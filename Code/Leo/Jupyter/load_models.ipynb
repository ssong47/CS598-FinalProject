{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import pyrealsense2 as rs\n",
    "import serial\n",
    "import time \n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60, 80, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 60, 80, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 60, 80, 16)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 30, 40, 16)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 30, 40, 32)   4640        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 30, 40, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 15, 20, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 15, 20, 64)   18496       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 20, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 7, 10, 64)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 10, 96)    55392       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7, 10, 96)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 3, 5, 96)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          700         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1440)         0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          184448      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 228)          0           activation_6[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          22900       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            101         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "thetaz_out (Activation)         (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 297,225\n",
      "Trainable params: 297,225\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = r'C:\\Users\\77bis\\Desktop\\CS598-FinalProject\\Code\\Leo\\alex_saved_model'\n",
    "new_model = tf.keras.models.load_model(saved_model_path)\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with full model\n",
    "\n",
    "Full model takes approximately 24ms to run for each stream of force and image data, with error of 3.7 deg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_start = 1\n",
    "i_end = 20000\n",
    "\n",
    "\n",
    "# Setup standardization\n",
    "sc_X2 = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "\n",
    "# Read force data \n",
    "force_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\fcss_processed_leo_test35_11_25_2020.txt'\n",
    "f_data_raw = pd.read_csv(force_file_path)\n",
    "f_data_all = sc_X2.fit_transform(f_data_raw.values)\n",
    "f_data_i = f_data_all[i_start:i_end, :]\n",
    "\n",
    "\n",
    "# Read ground truth data\n",
    "qtm_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\qtm_processed_leo_test35_11_25_2020.txt'\n",
    "qtm_data_raw = pd.read_csv(qtm_file_path)\n",
    "qtm_data_all = sc_y.fit_transform(qtm_data_raw.values[:,0].reshape(-1,1))\n",
    "qtm_data_i = qtm_data_all[i_start:i_end, :]\n",
    "\n",
    "img_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\depth_processed_leo_test35.avi'\n",
    "cap = cv2.VideoCapture(img_file_path)\n",
    "image_data_i = []\n",
    "\n",
    "for data_idx in range(i_start, i_end):\n",
    "    # Read depth image data\n",
    "    cap.set(1, data_idx-1)\n",
    "    res, frame = cap.read()\n",
    "    gray_frame = frame/255.0\n",
    "    ex_img_data = cv2.resize(gray_frame, \\\n",
    "                                        (80, 60),\\\n",
    "                                        interpolation = cv2.INTER_NEAREST)\n",
    "    image_data_i.append(ex_img_data) \n",
    "\n",
    "\n",
    "y_pred = new_model.predict([np.array(image_data_i), f_data_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit y_pred = new_model.predict([np.array(image_data_i), f_data_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = sc_y.inverse_transform(y_pred)\n",
    "y_truth_2 = sc_y.inverse_transform(qtm_data_i)\n",
    "N = len(qtm_data_i)\n",
    "plot_i = np.arange(0, N)\n",
    "\n",
    "truth = plt.plot(plot_i, y_truth_2, label = 'truth')\n",
    "pred = plt.plot(plot_i, y_pred_2, label = 'pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "avg_e = (abs(y_truth_2 - y_pred_2))\n",
    "print(avg_e.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert full model to lite model \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "\n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "\n",
    "# Save lite model\n",
    "lite_model_path = r'C:\\Users\\77bis\\Desktop\\CS598-FinalProject\\Code\\Leo\\test_lite_model'\n",
    "with open(lite_model_path, \"wb\") as f:\n",
    "    f.write(lite_model_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret lite model \n",
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "\n",
    "\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(image, force):\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], image)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[1]['index'], force)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tensorflow Lite model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lite model\n",
    "lite_model_path = r'C:\\Users\\77bis\\Desktop\\CS598-FinalProject\\Code\\Leo\\test_lite_model'\n",
    "\n",
    "\n",
    "# Interpret lite model \n",
    "interpreter = tf.lite.Interpreter(model_path = lite_model_path)\n",
    "\n",
    "\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(image, force):\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], image)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[1]['index'], force)\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Lite Model using existing Dataset\n",
    "Lite model takes about 1ms to run each stream of force and image data with error of 3.5 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_xy(f_data):\n",
    "    d1 = 0.19\n",
    "    d2 = 0.08\n",
    "    d3 = 0.19\n",
    "    \n",
    "    offset_x = -0.8768\n",
    "    slope_x = -0.4964\n",
    "\n",
    "    offset_y = 31.2436\n",
    "    slope_y = -0.4404\n",
    "    \n",
    "    g = 9.81\n",
    "    \n",
    "    Fz1 = f_data[0] \n",
    "    Fz2 = f_data[1]\n",
    "    Fz3 = f_data[2]\n",
    "    Fx1 = f_data[3]\n",
    "    Fx2 = f_data[4]\n",
    "    Fy = f_data[5]\n",
    "    \n",
    "    Mx = ((Fz2 - Fz3) * d3 + Fy * d2) * g\n",
    "    My = ((Fz2 + Fz3 - Fz1) * d1 + (Fx1 + Fx2) * d2) * g\n",
    "\n",
    "    theta_x = offset_x + slope_x * Mx\n",
    "    theta_y = offset_y + slope_y * My\n",
    "\n",
    "\n",
    "    return theta_x, theta_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.904659594800004, -0.23687285840000527)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_angle_xy(ex_f_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data_i_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setup standardization\n",
    "sc_X2 = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# Read force data \n",
    "force_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\fcss_processed_leo_test35_11_25_2020.txt'\n",
    "f_data_raw = pd.read_csv(force_file_path)\n",
    "f_data_all = sc_X2.fit_transform(f_data_raw.values)\n",
    "f_data_all_raw = f_data_raw.values\n",
    "\n",
    "\n",
    "N = len(f_data_all)\n",
    "i_start = 1\n",
    "i_end = N\n",
    "\n",
    "f_data_i = f_data_all[i_start:i_end, :]\n",
    "\n",
    "\n",
    "# Read ground truth data\n",
    "qtm_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\qtm_processed_leo_test35_11_25_2020.txt'\n",
    "qtm_data_raw = pd.read_csv(qtm_file_path)\n",
    "qtm_data_all = sc_y.fit_transform(qtm_data_raw.values[:,0].reshape(-1,1))\n",
    "qtm_data_i = qtm_data_raw.values[i_start:i_end, :]\n",
    "\n",
    "img_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\depth_processed_leo_test35.avi'\n",
    "cap = cv2.VideoCapture(img_file_path)\n",
    "\n",
    "# prediction for ML\n",
    "y_pred_x = np.zeros((i_end - i_start,1))\n",
    "y_pred_y = np.zeros((i_end - i_start,1))\n",
    "y_pred_z = np.zeros((i_end - i_start,1))\n",
    "\n",
    "\n",
    "# Save prediction\n",
    "f = open(r\"C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\ml_pred_leo_test35.txt\", \"x\")\n",
    "\n",
    "for data_idx in range(i_start, i_end):\n",
    "    ex_f_data_raw = f_data_all_raw[data_idx-2,:]\n",
    "    ex_f_data = f_data_all[data_idx-2,:]\n",
    "\n",
    "\n",
    "    # Read depth image data\n",
    "    cap.set(1, data_idx-1)\n",
    "    res, frame = cap.read()\n",
    "    gray_frame = frame/255.0\n",
    "    ex_img_data = cv2.resize(gray_frame, \\\n",
    "                                        (80, 60),\\\n",
    "                                        interpolation = cv2.INTER_NEAREST)\n",
    "    # cv2.imshow('s',ex_img_data)\n",
    "    # cv2.waitKey(0)  \n",
    "    # cv2.destroyAllWindows()  \n",
    "\n",
    "    ex_qtm_data = qtm_data_all[data_idx - 2]\n",
    "    \n",
    "    y_pred_temp = lite_model([np.array(ex_img_data, dtype='float32')], [np.array(ex_f_data, dtype='float32')])[0]\n",
    "    \n",
    "    y_pred_z[data_idx-i_start] = y_pred_temp[0]\n",
    "    \n",
    "    theta_x, theta_y = compute_angle_xy(ex_f_data_raw)\n",
    "    \n",
    "    y_pred_x[data_idx - i_start]  = theta_x\n",
    "    y_pred_y[data_idx - i_start]  = theta_y\n",
    "    \n",
    "    f.write(str(theta_x) + ',' + str(theta_y) + ',' + str(sc_y.inverse_transform(y_pred_temp)[0]) + '\\n')\n",
    "\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.798475485935807\n"
     ]
    }
   ],
   "source": [
    "y_pred_z_2 = sc_y.inverse_transform(y_pred_z)\n",
    "y_truth_z_2 = qtm_data_i[:,0]\n",
    "N = len(qtm_data_i)\n",
    "plot_i = np.arange(0, N)\n",
    "\n",
    "plt.figure(1)\n",
    "truth = plt.plot(plot_i, y_truth_z_2, label = 'truth')\n",
    "pred = plt.plot(plot_i, y_pred_z_2, label = 'pred')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "y_truth_x_2 = qtm_data_i[:,2]\n",
    "\n",
    "plt.figure(2)\n",
    "truth = plt.plot(plot_i, y_truth_x_2, label = 'truth')\n",
    "pred = plt.plot(plot_i, y_pred_x, label = 'pred')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "y_truth_y_2 = qtm_data_i[:,1]\n",
    "\n",
    "plt.figure(3)\n",
    "truth = plt.plot(plot_i, y_truth_y_2, label = 'truth')\n",
    "pred = plt.plot(plot_i, y_pred_y, label = 'pred')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "avg_e = (abs(y_truth_z_2 - y_pred_z_2))\n",
    "print(avg_e.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Live Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setup:\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "cfg.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "\n",
    "# Create an object to read  \n",
    "# from camera \n",
    "video = cv2.VideoCapture(0) \n",
    "   \n",
    "# We need to check if camera \n",
    "# is opened previously or not \n",
    "if (video.isOpened() == False):  \n",
    "    print(\"Error reading video file\") \n",
    "    \n",
    "# We need to set resolutions. \n",
    "# so, convert them from float to integer. \n",
    "frame_width = int(video.get(3)) \n",
    "frame_height = int(video.get(4)) \n",
    "   \n",
    "size = (frame_width, frame_height) \n",
    "\n",
    "\n",
    "\n",
    "# Below VideoWriter object will create \n",
    "# a frame of above defined The output  \n",
    "# is stored in 'filename.avi' file. \n",
    "colorizer = rs.colorizer()\n",
    "colorizer.set_option(rs.option.color_scheme, 3)\n",
    "mag_deci = 4\n",
    "decimation = rs.decimation_filter(mag_deci)\n",
    "threshold = rs.threshold_filter(0.15, 1.6)\n",
    "\n",
    "# n_trial = 30\n",
    "# depthwriter = cv2.VideoWriter('fcss_leo_depth_test'+str(n_trial)+'.avi',  \n",
    "#                          cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "#                          30, (int(frame_width/mag_deci), int(frame_height/mag_deci)), 0) \n",
    "\n",
    "\n",
    "print(\"type anything to start\")\n",
    "start_test = input()\n",
    "\n",
    "\n",
    "addr = \"COM4\" ## serial port to read data from\n",
    "baud = 115200 ## baud rate for instrument\n",
    "\n",
    "ser = serial.Serial(\n",
    "    port = addr,\\\n",
    "    baudrate = baud)\n",
    "\n",
    "# fcss_filename = \"fcss_leo_load_test\"+str(n_trial)+\".csv\"\n",
    "# datafile = open(fcss_filename, 'a')\n",
    "\n",
    "\n",
    "sc_X2 = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "qtm_file_path = r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5\\Test_Subject_leo\\test35\\qtm_processed_leo_test35_11_25_2020.txt'\n",
    "qtm_data_raw = pd.read_csv(qtm_file_path)\n",
    "qtm_data_all = sc_y.fit_transform(qtm_data_raw.values[:,0].reshape(-1,1))\n",
    "\n",
    "\n",
    "# Start streaming\n",
    "pipe.start(cfg)\n",
    "start_time = cv2.getTickCount()\n",
    "prev_time = 0\n",
    "count = 0\n",
    "count_total = 0 \n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "#         print((cv2.getTickCount() - start_time)/cv2.getTickFrequency())\n",
    "#         try:\n",
    "            prev_time = cv2.getTickCount()\n",
    "            ser_bytes = ser.readline()\n",
    "            decoded_bytes = (ser_bytes[0:len(ser_bytes)-2].decode(\"utf-8\").split(\",\"))\n",
    "\n",
    "            write_line = \",\".join(decoded_bytes) + str(\"\\n\")\n",
    "            values = [float(i) for i in write_line.split(',')]\n",
    "            force_data = sc_X2.fit_transform(np.array(values[1:], dtype = 'float32').reshape(-1,1))\n",
    "#             datafile.write(write_line)\n",
    "            \n",
    "            \n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipe.wait_for_frames()\n",
    "            depth_frame = frames.get_depth_frame()\n",
    "            \n",
    "            if not depth_frame:\n",
    "                continue\n",
    "            \n",
    "            # Decimation \n",
    "            depth_frame = decimation.process(depth_frame)\n",
    "\n",
    "            # Thresholding\n",
    "            depth_frame = threshold.process(depth_frame)\n",
    "        \n",
    "            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "            depth_colormap = np.asanyarray(colorizer.colorize(depth_frame).get_data())     \n",
    "            key = cv2.waitKey(1)\n",
    "            \n",
    "            # Resize the CV frame to fit the decimated frame\n",
    "            depth_data = cv2.resize(depth_colormap, (80, 60))/255\n",
    "            \n",
    "         \n",
    "            # Save depth camera frame \n",
    "#             depthwriter.write(depth_save)\n",
    "            \n",
    "            # Show depth camera \n",
    "#             cv2.imshow('Depth', depth_data)\n",
    "            \n",
    "            y_pred_temp = lite_model([np.array(depth_data, dtype='float32')], [force_data[:,0]])[0]\n",
    "            print(sc_y.inverse_transform(y_pred_temp))\n",
    "            \n",
    "            # Press esc or 'q' to close the image window\n",
    "            if key & 0xFF == ord('q') or key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                video.release() \n",
    "#                 depthwriter.release()\n",
    "                break \n",
    "#             count_total = count_total+ 1  \n",
    "#             el_t = (cv2.getTickCount() - prev_time)/cv2.getTickFrequency()\n",
    "#             if el_t > 0.04:\n",
    "#                 count = count + 1\n",
    "#                 print(el_t)\n",
    "#             prev_time = cv2.getTickCount()\n",
    "            \n",
    "                \n",
    "#         except:\n",
    "#             pass\n",
    "#             break\n",
    "\n",
    "finally:\n",
    "    ser.close()\n",
    "#     datafile.close()\n",
    "    pipe.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release() \n",
    "#     depthwriter.release()\n",
    "#     print(count/count_total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_temp = lite_model([np.array(depth_data, dtype='float32')], [force_data[:,0]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"demofile2.txt\", \"a\")\n",
    "f.write(\"Now the file has more content!\")\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
