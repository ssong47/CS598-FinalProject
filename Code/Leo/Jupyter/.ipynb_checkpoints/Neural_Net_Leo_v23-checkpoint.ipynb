{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.svm import SVR\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from scipy import stats\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed value for random number generator for repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_value = 9\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)# 5. Configure a new global `tensorflow` session\n",
    "\n",
    "# TODO need to add session thing for tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input Depth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading depth camera\n",
    "def read_depth_camera(dcamera_path, show_video, nw_resize=1, nh_resize=1):\n",
    "    video  = cv2.VideoCapture(dcamera_path)\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    # Get total # of frame count \n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    frame_height = int(frame.shape[0])\n",
    "    frame_width = int(frame.shape[1])\n",
    "\n",
    "    \n",
    "    depth_frames = np.empty((frame_count, int(frame_height/nh_resize), int(frame_width/nw_resize)))\n",
    "    depth_frames = np.empty((frame_count, int(frame_height/nh_resize), int(frame_width/nw_resize),3))\n",
    "    count = 0\n",
    "    while (video.isOpened()):\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if ret == True:\n",
    "            gray_frame = frame\n",
    "            gray_frame = cv2.resize(gray_frame, \\\n",
    "                                    (int(frame_width/nw_resize), int(frame_height/nh_resize)),\\\n",
    "                                    interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "            depth_frames[count] = gray_frame\n",
    "            if show_video == True:\n",
    "                cv2.imshow(\"Depth\", gray_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            count = count + 1\n",
    "        else: \n",
    "            break\n",
    "            \n",
    "\n",
    "    video.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    return depth_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "nw_resize = 2 # for reducing width\n",
    "nh_resize = 2 # for reducing height\n",
    "xtemp = {}\n",
    "show_video = 0\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "        \n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    train_dcamera_path = os.path.join(data_dir , 'depth_processed_'+subj[i]+'_test'+str(n_test[i])+'.avi')\n",
    "    xtemp[i] = read_depth_camera(train_dcamera_path, show_video, nw_resize=nw_resize, nh_resize=nh_resize).astype('uint8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen=0 # total length of training data set\n",
    "for x in range(len(xtemp)):\n",
    "    tlen+= xtemp[x].shape[0]\n",
    "\n",
    "\n",
    "x_train = np.zeros((tlen,xtemp[0].shape[1],xtemp[0].shape[2],xtemp[0].shape[3]),dtype='uint8') # initialize training set data\n",
    "xrun_cum = 0\n",
    "for i in range (len(xtemp)):\n",
    "    xrun_n = len(xtemp[i])\n",
    "    x_train[xrun_cum:xrun_cum+xrun_n,:,:,:] = xtemp[i][:xrun_n,:,:,:] # compiling all the training data into one large array\n",
    "    xrun_cum += xrun_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input FDSS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "date = ('11_15_2020','11_24_2020','11_24_2020','11_25_2020','11_25_2020','11_25_2020')\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "subjwgt = [67, 67, 67, 67, 67, 67]\n",
    "subjht = [174, 174, 174, 174, 174, 174]\n",
    "xfcss_gt = {}\n",
    "yrun = 0\n",
    "\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    fcss_data_dir = os.path.join(data_dir , 'fcss_processed_'+subj[i]+'_' + test_str + '_' + date[i] + '.txt')\n",
    "    xfcss_gttemp = pd.read_csv(fcss_data_dir)/subjwgt[i]\n",
    "    xfcss_gt[i]=xfcss_gttemp\n",
    "    if i == 0:\n",
    "        xfcss_train = xfcss_gttemp\n",
    "    else:\n",
    "        xfcss_train = pd.concat([xfcss_train,xfcss_gt[i]],axis=0)\n",
    "del xfcss_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Saturate Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_data(qtm_file_data, theta):\n",
    "    if theta=='x':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Lean Left/Right Angle (deg)\"])\n",
    "    if theta=='y':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Lean Forward/Backwards Angle (deg)\"])\n",
    "    if theta=='z':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Torso Twist Angle (deg)\"])\n",
    "        \n",
    "    \n",
    "    return qtm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to saturate the theta to minimum and maximum values of the entire class. \n",
    "# This only applies to cases when the desired class's minimum and maximum are below the observed minimum and maximum\n",
    "\n",
    "def saturate(theta, min_val, max_val):\n",
    "    for i in range(len(theta)):\n",
    "        if theta[i] < min_val:\n",
    "            theta[i] = min_val\n",
    "            continue\n",
    "        if theta[i] > max_val:\n",
    "            theta[i] = max_val\n",
    "            continue\n",
    "    return theta\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "date = ('11_15_2020','11_24_2020','11_24_2020','11_25_2020','11_25_2020','11_25_2020')\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "y_gt = {}\n",
    "yrun = 0\n",
    "theta_interest = 'z'\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    qtm_file_data_dir = os.path.join(data_dir , 'qtm_processed_'+subj[i]+'_test' + str(n_test[i]) + '_' + date[i] + '.txt')\n",
    "    y_gt[i] = read_output_data(qtm_file_data_dir,theta_interest).values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen=0\n",
    "for x in range(len(y_gt)):\n",
    "    tlen+= y_gt[x].shape[0]\n",
    "    \n",
    "yrun_cum = 0\n",
    "y_train = np.zeros((tlen,1))\n",
    "for i in range (len(y_gt)):\n",
    "    yrun_n = len(y_gt[i])\n",
    "    y_train[yrun_cum:yrun_cum+yrun_n] = y_gt[i][:]\n",
    "    yrun_cum += yrun_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = saturate(y_train, -50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide Data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = x_train.shape[0]\n",
    "n80p = int(np.floor(nsamps*0.8))\n",
    "\n",
    "Trainset = x_train[0:n80p]\n",
    "Trainset2 = xfcss_train.values[0:n80p]\n",
    "Trainy = y_train[0:n80p]\n",
    "\n",
    "Testset = x_train[n80p:nsamps]\n",
    "Testset2 = xfcss_train.values[n80p:nsamps]\n",
    "Testy = y_train[n80p:nsamps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize input and output, Discretize Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X2 = StandardScaler()\n",
    "sc_y = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bin = 50\n",
    "min_val = -5\n",
    "max_val = 5\n",
    "\n",
    "X = Trainset/255.0\n",
    "X2 = sc_X2.fit_transform(Trainset2)\n",
    "\n",
    "\n",
    "\n",
    "# y_temp = sc_y.fit_transform(np.array(Trainy).reshape(-1,1))\n",
    "normalize_y_val = max(np.array(Trainy).reshape(-1,1))\n",
    "y_temp = np.array(Trainy).reshape(-1,1)/normalize_y_val\n",
    "\n",
    "y_bins = np.linspace(min_val, max_val, n_bin)\n",
    "y_binned = np.digitize(y_temp, y_bins)\n",
    "y = (y_binned - n_bin/2)/n_bin * max_val*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(y_temp, label = 'raw')\n",
    "plt.plot(y, label = 'discretized')\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make validation data available to model.fit\n",
    "Xvalid = Testset/255.0\n",
    "Xvalid2 = sc_X2.transform(Testset2)\n",
    "y_valid_temp = Testy/normalize_y_val\n",
    "y_valid_binned = np.digitize(y_valid_temp, y_bins)\n",
    "y_valid = (y_valid_binned - n_bin/2)/n_bin * max_val*2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "plt.plot(y_valid_temp, label = 'raw')\n",
    "plt.plot(y_valid, label = 'discretized')\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Netowrk\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Conv2D, MaxPooling2D, Input, concatenate, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Reshape, Permute, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model_start = Input(shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]))\n",
    "model_start2 = Input(shape=(xfcss_train.shape[1],))\n",
    "model_cnn = model_start\n",
    "model_perc = model_start2\n",
    "\n",
    "model_cnn = Conv2D(filters=8, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "# model_cnn = BatchNormalization()(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_perc = Dense(32)(model_perc)\n",
    "model_perc = Activation('relu')(model_perc)\n",
    "\n",
    "model_cnn = Conv2D(filters=16, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "# model_cnn = BatchNormalization()(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "# model_perc = Dense(32)(model_perc)\n",
    "# model_perc = Activation('relu')(model_perc)\n",
    "\n",
    "model_cnn = Conv2D(filters=32, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "# model_cnn = BatchNormalization()(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_cnn = Conv2D(filters=64, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "# model_cnn = BatchNormalization()(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_cnn = Flatten()(model_cnn)\n",
    "# model_perc = Flatten()(model_perc)\n",
    "# model_cnn = Activation('relu')(model_cnn)\n",
    "\n",
    "# model_cnn = Dense(128)(model_cnn)\n",
    "# model_cnn = Activation('relu')(model_cnn)\n",
    "# model_cnn = Dropout(dropout_rate)(model_cnn)\n",
    "\n",
    "model_comb = concatenate([model_cnn,model_perc],axis=-1)\n",
    "\n",
    "model_comb = Dense(256)(model_comb)\n",
    "# model_comb = BatchNormalization()(model_comb)\n",
    "model_comb = Activation('relu')(model_comb)\n",
    "model_comb = Dropout(dropout_rate)(model_comb)\n",
    "\n",
    "output = Dense(1)(model_comb)\n",
    "output = Activation('linear', name='thetaz_out')(output)\n",
    "model = Model(inputs=[model_start,model_start2],outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mae',\n",
    "              metrics=['mse','mae'])\n",
    "\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True) #Moving to 1000 patience. \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50,restore_best_weights=True) #Moving to 1000 patience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 60, 80, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 60, 80, 8)    224         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 60, 80, 8)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 30, 40, 8)    0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 30, 40, 16)   1168        average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 30, 40, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 15, 20, 16)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 20, 32)   4640        average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 20, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 10, 32)    0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 10, 64)    18496       average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 7, 10, 64)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 3, 5, 64)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           224         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 960)          0           average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 992)          0           flatten_1[0][0]                  \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          254208      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "thetaz_out (Activation)         (None, 1)            0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 279,217\n",
      "Trainable params: 279,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 - 83s - loss: 0.1352 - mse: 0.0945 - mae: 0.1352 - val_loss: 0.7428 - val_mse: 0.9562 - val_mae: 0.7428\n"
     ]
    }
   ],
   "source": [
    "epochs = int(1)\n",
    "batch_size = 256\n",
    "history = model.fit([X,X2], y, batch_size=batch_size, epochs = epochs,callbacks = [callback],validation_data = ([Xvalid,Xvalid2], y_valid),verbose=2)\n",
    "# history = model.fit_generator(get_generator_cyclic(Xtrainz, Xtrainz2, ytrainz, batch_size=batch_size))\n",
    "\n",
    "\n",
    "# model.save('depthforcemodel.h5')\n",
    "#history.save('depthforcehist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    \n",
    "    v1 = history.history['mae']*np.sqrt(sc_y.var_)\n",
    "    v2 = history.history['val_mae']*np.sqrt(sc_y.var_)\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(v1, label='mae')\n",
    "    ax1.plot(v2, label='val_mae')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Error [deg]')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    v3 = history.history['mse']*sc_y.var_\n",
    "    v4 = history.history['val_mse']*sc_y.var_\n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    ax2.plot(v3, label='mse')\n",
    "    ax2.plot(v4, label='val_mse')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Error [deg^2]')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check with 80% data\n",
    "Xtrainz = Trainset/255.\n",
    "Xtrainz2 = sc_X2.transform(Trainset2)\n",
    "y_pred = model.predict([Xtrainz,Xtrainz2]) * normalize_y_val\n",
    "y_new = Trainy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.13 degrees\n",
      "Root Mean Squared Error is 0.27 degrees\n",
      "Mean Absolute Error is 0.27 degrees\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(y_new,'k')\n",
    "plt.plot(y_pred,'r--')\n",
    "plt.title('Prediction of Training Set (Sanity Check)')\n",
    "#plt.axis([xmin, xmax, ymin, ymax])\n",
    "plt.legend(labels=['Ground Truth','Prediction'])\n",
    "plt.show()\n",
    "# Squared-root of Squared Error\n",
    "\n",
    "test_error = (y_pred - y_new)\n",
    "print('Average error is {:4.2f} degrees'.format(np.sum(test_error)/test_error.shape[0]))\n",
    "rmse = np.sqrt(test_error**2)\n",
    "print('Root Mean Squared Error is {:4.2f} degrees'.format(np.sum(rmse)/test_error.shape[0]))\n",
    "# Mean absolute error\n",
    "print('Mean Absolute Error is {:4.2f} degrees'.format(np.sum(np.abs(test_error))/test_error.shape[0]))\n",
    "# plt.figure(figsize=(20,6))\n",
    "# plt.plot(rmse,'.')\n",
    "# plt.title('Sqrt(Squared Error) of Training Set (Sanity Check)')\n",
    "# plt.xlabel('Sample')\n",
    "# plt.ylabel('Error (degrees)')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(test_error,bins=100)\n",
    "# plt.title('Histogram of Residuals in Training Set')\n",
    "# plt.xlabel('Angle')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# #plot scatterplot of data\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.scatter(y_pred,y_new,marker='.',color='black')\n",
    "# plt.xlabel('Predicted angle (degrees)')\n",
    "# plt.ylabel('Ground truth angle (degrees)')\n",
    "# plt.title('Ground truth vs predicted angle of Training Set')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (47,49)\n",
    "subj = ['leo','leo']\n",
    "date = ('12_15_2020','12_15_2020')\n",
    "subjwgt = [67, 67]\n",
    "subjht = [174, 174]\n",
    "nw_resize = 2 # for reducing width\n",
    "nh_resize = 2 # for reducing height\n",
    "xtemp = {}\n",
    "show_video = 0\n",
    "    \n",
    "    \n",
    "# READING DEPTH DATA \n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5 Incline','Test_Subject_'+subj[i],test_str)\n",
    "    test_dcamera_path = os.path.join(data_dir , 'depth_processed_'+subj[i]+'_test'+str(n_test[i])+'.avi')\n",
    "    xtemp[i] = read_depth_camera(test_dcamera_path, show_video, nw_resize=nw_resize, nh_resize=nh_resize).astype('uint8')\n",
    "    \n",
    "\n",
    "tlen=0 # total length of training data set\n",
    "for x in range(len(xtemp)):\n",
    "    tlen+= xtemp[x].shape[0]\n",
    "\n",
    "x_test = np.zeros((tlen,xtemp[0].shape[1],xtemp[0].shape[2],xtemp[0].shape[3]),dtype='uint8') # initialize training set data\n",
    "xrun_cum = 0\n",
    "for i in range (len(xtemp)):\n",
    "    xrun_n = len(xtemp[i])\n",
    "    x_test[xrun_cum:xrun_cum+xrun_n,:,:,:] = xtemp[i][:xrun_n,:,:,:] # compiling all the training data into one large array\n",
    "    xrun_cum += xrun_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING FDSS DATA\n",
    "xfcss_gt = {}\n",
    "yrun = 0\n",
    "\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5 Incline','Test_Subject_'+subj[i],test_str)\n",
    "    fcss_data_dir = os.path.join(data_dir , 'fcss_processed_'+subj[i]+'_' + test_str + '_' + date[i] + '.txt')\n",
    "    xfcss_gttemp = pd.read_csv(fcss_data_dir)/subjwgt[i]\n",
    "    xfcss_gt[i]=xfcss_gttemp\n",
    "    if i == 0:\n",
    "        xfcss_test = xfcss_gttemp\n",
    "    else:\n",
    "        xfcss_test = pd.concat([xfcss_test,xfcss_gt[i]],axis=0)\n",
    "del xfcss_gt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING OUTPUT DATA \n",
    "y_gt = {}\n",
    "yrun = 0\n",
    "theta_interest = 'z'\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5 Incline','Test_Subject_'+subj[i],test_str)\n",
    "    qtm_file_data_dir = os.path.join(data_dir , 'qtm_processed_'+subj[i]+'_test' + str(n_test[i]) + '_' + date[i] + '.txt')\n",
    "    y_gt[i] = read_output_data(qtm_file_data_dir,theta_interest).values\n",
    "\n",
    "tlen=0\n",
    "for x in range(len(y_gt)):\n",
    "    tlen+= y_gt[x].shape[0]\n",
    "    \n",
    "yrun_cum = 0\n",
    "y_test = np.zeros((tlen,1))\n",
    "for i in range (len(y_gt)):\n",
    "    yrun_n = len(y_gt[i])\n",
    "    y_test[yrun_cum:yrun_cum+yrun_n] = y_gt[i][:]\n",
    "    yrun_cum += yrun_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo test set\n",
    "Xtest = x_test/255.\n",
    "Xtest2 = sc_X2.transform(xfcss_test)\n",
    "y_test_pred = model.predict([Xtest,Xtest2])\n",
    "y_test_pred = sc_y.inverse_transform(y_test_pred)\n",
    "y_test_new = y_test/max(y_test) * max_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error is 0.13 degrees\n",
      "Root Mean Squared Error is 0.27 degrees\n",
      "Mean Absolute Error is 0.27 degrees\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(y_test_new,'k')\n",
    "plt.plot(y_test_pred,'r--')\n",
    "plt.title('Prediction of Test')\n",
    "#plt.axis([xmin, xmax, ymin, ymax])\n",
    "plt.legend(labels=['Ground Truth','Prediction'])\n",
    "plt.show()\n",
    "\n",
    "# Squared-root of Squared Error\n",
    "\n",
    "test_error = (y_pred - y_new)\n",
    "print('Average error is {:4.2f} degrees'.format(np.sum(test_error)/test_error.shape[0]))\n",
    "rmse = np.sqrt(test_error**2)\n",
    "print('Root Mean Squared Error is {:4.2f} degrees'.format(np.sum(rmse)/test_error.shape[0]))\n",
    "# Mean absolute error\n",
    "print('Mean Absolute Error is {:4.2f} degrees'.format(np.sum(np.abs(test_error))/test_error.shape[0]))\n",
    "# plt.figure(figsize=(20,6))\n",
    "# plt.plot(rmse,'.')\n",
    "# plt.title('Sqrt(Squared Error) of Test Set')\n",
    "# plt.xlabel('Sample')\n",
    "# plt.ylabel('Error (degrees)')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(test_error,bins=100)\n",
    "# plt.title('Histogram of Residuals in Test Set')\n",
    "# plt.xlabel('Angle')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# #plot scatterplot of data\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.scatter(y_pred,y_new,marker='.',color='black')\n",
    "# plt.xlabel('Predicted angle (degrees)')\n",
    "# plt.ylabel('Ground truth angle (degrees)')\n",
    "# plt.title('Ground truth vs predicted angle')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
