{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Depth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading depth camera\n",
    "def read_depth_camera(dcamera_path, show_video, nw_resize=1, nh_resize=1):\n",
    "    video  = cv2.VideoCapture(dcamera_path)\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    # Get total # of frame count \n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    frame_height = int(frame.shape[0])\n",
    "    frame_width = int(frame.shape[1])\n",
    "\n",
    "    \n",
    "    depth_frames = np.empty((frame_count, int(frame_height/nh_resize), int(frame_width/nw_resize)))\n",
    "    depth_frames = np.empty((frame_count, int(frame_height/nh_resize), int(frame_width/nw_resize),3))\n",
    "    count = 0\n",
    "    while (video.isOpened()):\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        if ret == True:\n",
    "            gray_frame = frame\n",
    "            gray_frame = cv2.resize(gray_frame, \\\n",
    "                                    (int(frame_width/nw_resize), int(frame_height/nh_resize)),\\\n",
    "                                    interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "            depth_frames[count] = gray_frame\n",
    "            if show_video == True:\n",
    "                cv2.imshow(\"Depth\", gray_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            count = count + 1\n",
    "        else: \n",
    "            break\n",
    "            \n",
    "\n",
    "    video.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    return depth_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "nw_resize = 2 # for reducing width\n",
    "nh_resize = 2 # for reducing height\n",
    "xtemp = {}\n",
    "show_video = 0\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "        \n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    train_dcamera_path = os.path.join(data_dir , 'depth_processed_'+subj[i]+'_test'+str(n_test[i])+'.avi')\n",
    "    xtemp[i] = read_depth_camera(train_dcamera_path, show_video, nw_resize=nw_resize, nh_resize=nh_resize).astype('uint8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen=0 # total length of training data set\n",
    "for x in range(len(xtemp)):\n",
    "    tlen+= xtemp[x].shape[0]\n",
    "\n",
    "\n",
    "x_train = np.zeros((tlen,xtemp[0].shape[1],xtemp[0].shape[2],xtemp[0].shape[3]),dtype='uint8') # initialize training set data\n",
    "xrun_cum = 0\n",
    "for i in range (len(xtemp)):\n",
    "    xrun_n = len(xtemp[i])\n",
    "    x_train[xrun_cum:xrun_cum+xrun_n,:,:,:] = xtemp[i][:xrun_n,:,:,:] # compiling all the training data into one large array\n",
    "    xrun_cum += xrun_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Force Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "date = ('11_15_2020','11_24_2020','11_24_2020','11_25_2020','11_25_2020','11_25_2020')\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "subjwgt = [67, 67, 67, 67, 67, 67]\n",
    "subjht = [174, 174, 174, 174, 174, 174]\n",
    "xfcss_gt = {}\n",
    "yrun = 0\n",
    "\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    fcss_data_dir = os.path.join(data_dir , 'fcss_processed_'+subj[i]+'_' + test_str + '_' + date[i] + '.txt')\n",
    "    xfcss_gttemp = pd.read_csv(fcss_data_dir)/subjwgt[i]\n",
    "    xfcss_gt[i]=xfcss_gttemp\n",
    "    if i == 0:\n",
    "        xfcss_train = xfcss_gttemp\n",
    "    else:\n",
    "        xfcss_train = pd.concat([xfcss_train,xfcss_gt[i]],axis=0)\n",
    "del xfcss_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_moment(fdss_data):\n",
    "    d1 = 0.19\n",
    "    d2 = 0.08\n",
    "    d3 = 0.19\n",
    "    g = 9.81\n",
    "    \n",
    "    N = len(fdss_data)\n",
    "    moment = np.zeros((N,3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        moment[i,0] = ((fdss_data[i, 1] - fdss_data[i, 2]) * d3  + fdss_data[i,5] * d2)* g \n",
    "        \n",
    "        moment[i,1] = ((fdss_data[i, 1] + fdss_data[i, 2] - fdss_data[i,0]) * d1 + (fdss_data[i, 3] + fdss_data[i, 4]) * d2) * g\n",
    "        \n",
    "        moment[i,2] = (fdss_data[i,3] - fdss_data[i,4]) * d3 * g\n",
    "\n",
    "\n",
    "    return moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment = compute_moment(xfcss_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output_data(qtm_file_data, theta):\n",
    "    if theta=='x':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Lean Left/Right Angle (deg)\"])\n",
    "    if theta=='y':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Lean Forward/Backwards Angle (deg)\"])\n",
    "    if theta=='z':\n",
    "        qtm_data = pd.read_csv(qtm_file_data, usecols = [\"Torso Twist Angle (deg)\"])\n",
    "        \n",
    "    \n",
    "    return qtm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (24,30,31,32,33,35)\n",
    "date = ('11_15_2020','11_24_2020','11_24_2020','11_25_2020','11_25_2020','11_25_2020')\n",
    "subj = ['leo','leo','leo','leo','leo','leo']\n",
    "y_gt = {}\n",
    "yrun = 0\n",
    "theta_interest = 'z'\n",
    "for i in range(len(n_test)):\n",
    "    test_str = 'test' + str(n_test[i])\n",
    "    data_dir = os.path.join(r'C:\\Users\\77bis\\Box\\CS598 - Final Project\\Preliminary Data V5','Test_Subject_'+subj[i],test_str)\n",
    "    qtm_file_data_dir = os.path.join(data_dir , 'qtm_processed_'+subj[i]+'_test' + str(n_test[i]) + '_' + date[i] + '.txt')\n",
    "    y_gt[i] = read_output_data(qtm_file_data_dir,theta_interest).values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen=0\n",
    "for x in range(len(y_gt)):\n",
    "    tlen+= y_gt[x].shape[0]\n",
    "    \n",
    "yrun_cum = 0\n",
    "y_train = np.zeros((tlen,1))\n",
    "for i in range (len(y_gt)):\n",
    "    yrun_n = len(y_gt[i])\n",
    "    y_train[yrun_cum:yrun_cum+yrun_n] = y_gt[i][:]\n",
    "    yrun_cum += yrun_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into training, validation, and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = xfcss_train.shape[0]\n",
    "n60p = int(np.floor(nsamps*0.6))\n",
    "n80p = int(np.floor(nsamps*0.8))\n",
    "\n",
    "X = x_train[0:n60p]/255.\n",
    "TrainX2 = xfcss_train.values[0:n60p]\n",
    "TrainY = y_train[0:n60p]\n",
    "\n",
    "X_val = x_train[n60p:n80p]/255.\n",
    "ValX2 = xfcss_train.values[n60p:n80p]\n",
    "ValY = y_train[n60p:n80p]\n",
    "\n",
    "X_test = x_train[n80p:nsamps]/255.\n",
    "TestX2 = xfcss_train.values[n80p:nsamps]\n",
    "TestY = y_train[n80p:nsamps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = moment.shape[0]\n",
    "n60p = int(np.floor(nsamps*0.6))\n",
    "n80p = int(np.floor(nsamps*0.8))\n",
    "\n",
    "TrainX = moment[0:n60p]\n",
    "TrainY = y_train[0:n60p]\n",
    "\n",
    "ValX = moment[n60p:n80p]\n",
    "ValY = y_train[n60p:n80p]\n",
    "\n",
    "TestX = moment[n80p:nsamps]\n",
    "TestY = y_train[n80p:nsamps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22dd9887780>,\n",
       " <matplotlib.lines.Line2D at 0x22dd9887128>,\n",
       " <matplotlib.lines.Line2D at 0x22dd9887b38>,\n",
       " <matplotlib.lines.Line2D at 0x22dd9887f60>,\n",
       " <matplotlib.lines.Line2D at 0x22dd9887f98>,\n",
       " <matplotlib.lines.Line2D at 0x22dd98877b8>]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_x = np.zeros((6,1))\n",
    "X2 = np.zeros(TrainX2.shape)\n",
    "X2_val = np.zeros(ValX2.shape)\n",
    "X2_test = np.zeros(TestX2.shape)\n",
    "for i in range(6):\n",
    "    max_x[i] = max(abs(TrainX2[:,i]))\n",
    "    X2[:,i] = TrainX2[:,i]/max_x[i]\n",
    "    X2_val[:,i] = ValX2[:,i]/max_x[i]\n",
    "    X2_test[:,i] = TestX2[:,i]/max_x[i]\n",
    "    \n",
    "plt.plot(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretize Output Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-50.   0.  50.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ddc3b9be0>]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bin = 3\n",
    "min_val = -50\n",
    "max_val = 50\n",
    "\n",
    "y_bins = np.linspace(min_val, max_val, n_bin)\n",
    "print(y_bins)\n",
    "Y_raw = np.digitize(y, y_bins) - 1\n",
    "Y_val_raw = np.digitize(y_val, y_bins) - 1\n",
    "Y_test_raw = np.digitize(y_test, y_bins) -1 \n",
    "\n",
    "Y = tf.keras.utils.to_categorical(Y_raw, num_classes=n_bin-1, dtype='float32')\n",
    "Y_val = tf.keras.utils.to_categorical(Y_val_raw, num_classes=n_bin-1, dtype='float32')\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test_raw, num_classes=n_bin-1, dtype='float32')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(Y_val_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Netowrk\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Conv2D, MaxPooling2D, Input, concatenate, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Reshape, Permute, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model_start = Input(shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]))\n",
    "model_start2 = Input(shape=(xfcss_train.shape[1],))\n",
    "model_cnn = model_start\n",
    "model_perc = model_start2\n",
    "\n",
    "model_cnn = Conv2D(filters=8, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_perc = Dense(32)(model_perc)\n",
    "model_perc = Activation('relu')(model_perc)\n",
    "\n",
    "model_cnn = Conv2D(filters=16, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "\n",
    "model_cnn = Conv2D(filters=32, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_cnn = Conv2D(filters=64, kernel_size=(3, 3),padding='same')(model_cnn)\n",
    "model_cnn = Activation('relu')(model_cnn)\n",
    "model_cnn = AveragePooling2D(pool_size=(2, 2))(model_cnn)\n",
    "\n",
    "model_cnn = Flatten()(model_cnn)\n",
    "\n",
    "\n",
    "model_comb = concatenate([model_cnn,model_perc],axis=-1)\n",
    "\n",
    "model_comb = Dense(256)(model_comb)\n",
    "model_comb = Activation('relu')(model_comb)\n",
    "model_comb = Dropout(dropout_rate)(model_comb)\n",
    "\n",
    "output = Dense(n_bin-1)(model_comb)\n",
    "output = Activation('softmax', name='probability')(output)\n",
    "model = Model(inputs=[model_start,model_start2],outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50,restore_best_weights=True) #Moving to 1000 patience. \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50,restore_best_weights=True) #Moving to 1000 patience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 256)               15616     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 58,914\n",
      "Trainable params: 58,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Neural Netowrk\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Conv2D, MaxPooling2D, Input, concatenate, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Reshape, Permute, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape = (X.shape[1],)),\n",
    "    layers.Dense(256, activation = 'relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    layers.Dense(64, activation = 'relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation = 'relu'),\n",
    "    layers.Dense(n_bin-1)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#     loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50,restore_best_weights=True) #Moving to 1000 patience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49428, 2)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\77bis\\anaconda3\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:5095: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 65s 167ms/step - loss: 0.6077 - accuracy: 0.6640 - val_loss: 0.5826 - val_accuracy: 0.6918\n",
      "Epoch 2/10\n",
      "387/387 [==============================] - 58s 149ms/step - loss: 0.4361 - accuracy: 0.8047 - val_loss: 0.5414 - val_accuracy: 0.7332\n",
      "Epoch 3/10\n",
      "387/387 [==============================] - 59s 152ms/step - loss: 0.3909 - accuracy: 0.8316 - val_loss: 0.5536 - val_accuracy: 0.7346\n",
      "Epoch 4/10\n",
      "387/387 [==============================] - 59s 153ms/step - loss: 0.3684 - accuracy: 0.8394 - val_loss: 0.5557 - val_accuracy: 0.7279\n",
      "Epoch 5/10\n",
      "387/387 [==============================] - 58s 151ms/step - loss: 0.3438 - accuracy: 0.8515 - val_loss: 0.5918 - val_accuracy: 0.7302\n",
      "Epoch 6/10\n",
      "387/387 [==============================] - 59s 153ms/step - loss: 0.3219 - accuracy: 0.8609 - val_loss: 0.6193 - val_accuracy: 0.7179\n",
      "Epoch 7/10\n",
      "387/387 [==============================] - 58s 150ms/step - loss: 0.3004 - accuracy: 0.8702 - val_loss: 0.6156 - val_accuracy: 0.6858\n",
      "Epoch 8/10\n",
      "387/387 [==============================] - 58s 149ms/step - loss: 0.2928 - accuracy: 0.8739 - val_loss: 0.6860 - val_accuracy: 0.7161\n",
      "Epoch 9/10\n",
      "387/387 [==============================] - 57s 148ms/step - loss: 0.2773 - accuracy: 0.8804 - val_loss: 0.6928 - val_accuracy: 0.7114\n",
      "Epoch 10/10\n",
      "387/387 [==============================] - 61s 157ms/step - loss: 0.2595 - accuracy: 0.8882 - val_loss: 0.7259 - val_accuracy: 0.6838\n"
     ]
    }
   ],
   "source": [
    "a1 = time.perf_counter() \n",
    "history = model.fit([X, X2], Y, batch_size = 128, epochs = 10, callback = [callback],\n",
    "                    validation_data = ([X_val, X2_val], Y_val),verbose=1)\n",
    "a2 = time.perf_counter() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22deffdbfd0>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differences(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Lists of different length.\")\n",
    "    return sum(i != j for i, j in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.29768769]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict([X_test, X2_test])\n",
    "y_pred = np.argmax(y_pred_prob, axis = 1)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(Y_test_raw,'k')\n",
    "plt.plot(y_pred,'r--')\n",
    "plt.title('Prediction of Training Set (Sanity Check)')\n",
    "plt.legend(labels=['Ground Truth','Prediction'])\n",
    "plt.show()\n",
    "\n",
    "diff = differences(y_pred, Y_test_raw)\n",
    "print(diff/len(Y_test_raw) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
